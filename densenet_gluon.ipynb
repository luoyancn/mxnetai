{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet和ResNet的思路类似，最明显的区别就是，跨层\n",
    "# 的数据不再通过加法进行连接，而是通过concat进行拼接，\n",
    "# 确保上一层的信息可以完整的进入下一层当中\n",
    "\n",
    "# DenseNet的卷积快使用ResNet的改进版本BN->Relu->Conv\n",
    "# 每个卷积的输出通道数被称之为growth_rate\n",
    "# 假定输入为in_channels，并且有layers层数，则输出的\n",
    "# 通道数为in_channels + growth_rate * layers\n",
    "\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "def conv_block(channels):\n",
    "    out = nn.Sequential()\n",
    "    out.add(nn.BatchNorm(), nn.Activation('relu'), \n",
    "    nn.Conv2D(channels, kernel_size=3, padding=1))\n",
    "    return out\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Block):\n",
    "\n",
    "    def __init__(self, layers, growth_rate, *args, **kwargs):\n",
    "        super(DenseBlock, self).__init__(*args, **kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(layers):\n",
    "            self.net.add(conv_block(growth_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            out = layer(x)\n",
    "            x = nd.concat(x, out, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 23, 8, 8)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblk = DenseBlock(2, 10)\n",
    "dblk.initialize()\n",
    "x = nd.random_uniform(shape=(4, 3, 8, 8))\n",
    "dblk(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过渡块： 由于使用拼接的缘故，每一次dense之后，输出的通道数可能会激增\n",
    "# 为控制模型复杂度，引入过渡块，把输入的长宽减半，同时使用1×1的卷积来改变通道数\n",
    "\n",
    "def transition_block(channels):\n",
    "    out = nn.Sequential()\n",
    "    out.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(channels, kernel_size=1), \n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 10, 4, 4)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblk = transition_block(10)\n",
    "tblk.initialize()\n",
    "tblk(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet的主体就是交替串联使用稠密块（DenseBlock）和过渡块\n",
    "# 过渡层利用过渡块，每次将通道数减半\n",
    "\n",
    "init_channel = 64\n",
    "growth_rate = 32\n",
    "block_layers = [6, 12, 24, 16]\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "def dense_net():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Conv2D(init_channel, kernel_size=7, strides=2, padding=3),\n",
    "                nn.BatchNorm(), nn.Activation('relu'),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "        channels = init_channel\n",
    "        for i, layers in enumerate(block_layers):\n",
    "            net.add(DenseBlock(layers, growth_rate))\n",
    "            channels += layers * growth_rate\n",
    "            if len(block_layers) - 1 != i:\n",
    "                net.add(transition_block(channels // 2))\n",
    "\n",
    "        net.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "                nn.AvgPool2D(pool_size=1), nn.Flatten(),\n",
    "                nn.Dense(num_classes))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "import utils\n",
    "\n",
    "train_data, test_data = utils.load_data_fashion_mnist_new(batch_size=64, resize=32)\n",
    "ctx = utils.try_gpu()\n",
    "\n",
    "net = dense_net()\n",
    "net.initialize(ctx=ctx, init=init.Xavier())\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "utils.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=1)"
   ]
  }
 ]
}